{
  "Basics": {
    "#1": {
      "title": "Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning",
      "arxiv_id": "2501.15228",
      "doi": "",
      "pdf_url": "https://arxiv.org/pdf/2501.15228",
      "venue": "NeurIPS",
      "year": 2025,
      "authors": [
        "Yiqun Chen",
        "Lingyong Yan",
        "Weiwei Sun",
        "Xinyu Ma",
        "Yi Zhang",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Yiming Yang",
        "Jiaxin Mao"
      ],
      "institutions": [
        { "institution": "Renmin University of China", "abbr": "RUC", "full": "Renmin University of China" },
        { "institution": "Baidu Inc.", "abbr": "Baidu", "full": "Baidu Inc." },
        { "institution": "Carnegie Mellon University", "abbr": "CMU", "full": "Carnegie Mellon University" }
      ]
    }
  },

  "Core": {
    "rag_relevance": {
      "is_about_rag": true,
      "rag_type": [
        "Modularized RAG pipeline",
        "Agentic RAG pipeline"
      ],
      "primary_pipeline_stages": [
        "Query rewriting / query generation",
        "Retrieval",
        "Post-processing / filtering (document selection)",
        "Answer generation",
        "Agentic multi-step retrieval (via sub-question decomposition)"
      ],
      "complex_questions_support": {
        "supports": "yes",
        "why": "The Query Rewriter decomposes an initial question into multiple sub-questions, enabling multi-step retrieval for multi-hop QA."
      }
    },

    "core_problem": {
      "problem_statement": "RAG pipelines are typically optimized module-by-module (often via SFT), causing objective misalignment across query rewriting, selection, and generation; optimizing one module can hurt end-to-end answer quality.",
      "why_it_matters": [
        "End-to-end QA correctness depends on interdependent module behaviors (rewriting affects retrieval, which affects selection, which affects generation).",
        "Single-module RL methods often assume overly simple RAG pipelines and miss multi-module coordination."
      ]
    },

    "one_sentence_thesis": "MMOA-RAG treats a multi-module RAG pipeline as a cooperative multi-agent RL problem and jointly optimizes query rewriting, document selection, and answer generation using a shared outcome reward (answer F1), improving end-to-end QA performance.",

    "key_novelty": {
      "main_idea": "Cooperative Multi-Agent RL for Modular RAG (MMOA-RAG)",
      "explanation": [
        "Model each RAG module (Query Rewriter, Selector, Generator) as an RL agent with its own observation/prompt and action space, but trained to maximize a shared global reward.",
        "Use Multi-Agent PPO (MAPPO) to jointly optimize multiple LLM-based modules so their behaviors align toward final answer quality rather than local module objectives."
      ],
      "why_this_is_new_for_rag": [
        "Moves from independently optimized RAG modules to a cooperative learning formulation where coordination is explicitly trained end-to-end.",
        "Provides a general joint-optimization algorithm that can adapt to different modular configurations (2-agent vs 3-agent variants)."
      ]
    },

    "main_takeaways": [
      "Jointly optimizing multiple RAG modules as cooperative agents can yield consistent end-to-end QA gains over strong single-module or SFT-only baselines.",
      "A simple shared outcome reward (answer F1) plus lightweight per-module penalties is sufficient to stabilize multi-agent RL training for modular RAG.",
      "The framework is configurable (2-agent vs 3-agent) and remains effective across different retrievers (Contriever/BGE/E5)."
    ],

    "evaluation_highlights": [
      "Main setting (Contriever): HotpotQA F1 improves from best baseline 46.49 (RetRobust) to 48.29 (MMOA-RAG), +1.80.",
      "Main setting (Contriever): 2WikiMultihopQA F1 improves from best baseline 44.51 (RetRobust) to 46.40 (MMOA-RAG), +1.89.",
      "Main setting (Contriever): AmbigQA F1 improves from best baseline 45.92 (Rewrite-Retrieve-Read) to 48.59 (MMOA-RAG), +2.67.",
      "Warm-start vs joint RL (QR+S+G): HotpotQA F1 improves 44.69 → 48.29, +3.60 after MAPPO.",
      "Retriever robustness (E5): AmbigQA F1 improves 59.75 → 60.80, +1.05 vs best baseline.",
      "OOD: Training on HotpotQA and testing on AmbigQA yields F1 45.62 vs 44.08 (RetRobust), +1.54."
    ],

    "breakthrough_assessment": {
      "score_1_to_5": 4,
      "justification": "It provides a concrete and general multi-agent RL formulation for jointly optimizing multiple RAG modules with shared outcome reward, achieving consistent multi-dataset gains over strong RAG optimization baselines."
    }
  },

  "Methods_and_Evidence": {
    "system_pipeline": {
      "high_level_flow": [
        "Input question q",
        "Query Rewriter generates sub-questions {q_i}",
        "Retriever (fixed dense retriever) retrieves candidate documents for each q_i",
        "Selector filters candidates into a smaller evidence set D*",
        "Generator produces final answer â based on D*",
        "Compute shared reward from â vs gold answer; update all agents with MAPPO"
      ],
      "modules": [
        {
          "module": "Query Rewriter (agent)",
          "role": "Decompose complex/ambiguous question into sub-questions for multi-step retrieval.",
          "model": "LLM (shared backbone via prompts)",
          "inputs": ["question q", "rewriter prompt"],
          "outputs": ["sub-questions {q_i}"],
          "notes": "Penalty discourages generating too many sub-questions."
        },
        {
          "module": "Retriever (environment / fixed)",
          "role": "Retrieve candidate documents for each sub-question.",
          "model": "Contriever (main setting); also evaluated with BGE and E5 retrievers.",
          "inputs": ["sub-questions {q_i}"],
          "outputs": ["candidate documents D"]
        },
        {
          "module": "Selector (agent)",
          "role": "Select helpful document IDs from candidate set for grounding the generator.",
          "model": "LLM (shared backbone via prompts)",
          "inputs": ["question q", "candidate documents D", "selector prompt"],
          "outputs": ["selected doc IDs / subset D*"],
          "notes": "Action space is constrained to ID tokens/format to reduce exploration."
        },
        {
          "module": "Generator (agent)",
          "role": "Generate the final short answer using selected documents.",
          "model": "LLM (shared backbone via prompts)",
          "inputs": ["question q", "selected documents D*", "generator prompt"],
          "outputs": ["answer â"],
          "notes": "Penalty discourages overly long outputs."
        }
      ]
    },

    "technical_details": {
      "formalization": {
        "setting": "Cooperative multi-agent reinforcement learning (Co-MARL) over a modular RAG pipeline.",
        "objective": "Maximize shared outcome reward based on final answer quality (F1) while applying module-specific penalties for stability.",
        "constraints": "Retriever is fixed (treated as part of environment); learnable agents are LLM-based modules distinguished by prompts (parameter sharing)."
      },
      "inference_mechanism": [
        "Run Query Rewriter to produce sub-questions.",
        "Retrieve documents per sub-question using a fixed dense retriever to form candidates.",
        "Run Selector to output document IDs in a constrained format; map IDs to a selected subset.",
        "Run Generator to produce a brief final answer conditioned on the selected subset."
      ],
      "training_or_optimization": {
        "uses_post_training": true,
        "method": "Hybrid: warm-start SFT for each module + MAPPO joint optimization",
        "reward_function": [
          "Shared global reward: F1 score of the Generator's predicted answer vs ground truth (used for all agents).",
          "Query Rewriter penalty: -0.5 if number of sub-questions > 4, else 0 (discourage excessive decomposition).",
          "Selector penalty: -1 if output contains duplicate IDs or violates required format (e.g., 'Document0,Document3,...'), else 0.",
          "Generator penalty: -0.5 if generated answer exceeds a length threshold, else 0."
        ],
        "loss_functions": [
          "SFT loss for warm start (standard supervised fine-tuning objective).",
          "MAPPO actor-critic losses with GAE advantage estimation and PPO-style clipping."
        ],
        "data_construction": [
          "Query Rewriter SFT data: reuse publicly available query rewriting data from Rewrite-Retrieve-Read.",
          "Selector SFT labels: heuristic overlap—after lowercasing and removing stopwords/punctuation, if any token from (question ∪ gold answer) appears in a candidate document, include that document's ID as a positive label.",
          "Generator SFT labels: gold answers paired with selected documents, trained to output brief answer-only format."
        ]
      },
      "models_and_system_components": {
        "llm_backbone": "Llama-3-8B-Instruct (shared across Query Rewriter, Selector, Generator via different prompts).",
        "retriever_or_index": "Contriever as primary retriever; robustness tested with BGE and E5 retrievers.",
        "reranker": "Selector functions as a learned filtering step rather than a classical reranker.",
        "tooling_or_search_engine": "",
        "key_hyperparameters": {
          "top_k": "",
          "context_window": "",
          "other": "clip_range_MAPPO: 0.2, learning_rate_max: 2e-5, buffer_size: 128, gae_lambda: 1.0, gae_gamma: 0.95, sampling_temperature_or_top_p: 0.9, reward_clip_max: 0.2, reward_clip_min: 0.06"
        }
      }
    },

    "evaluation_setup": {
      "evaluation_setting": "Multi-hop and ambiguous QA with end-to-end metrics; comparison to multiple RAG optimization baselines under the same LLM backbone and retriever settings.",
      "benchmarks": [
        { "name": "HotpotQA", "task_type": "multi-hop QA", "newly_constructed": false },
        { "name": "2WikiMultihopQA", "task_type": "multi-hop QA", "newly_constructed": false },
        { "name": "AmbigQA", "task_type": "ambiguous QA", "newly_constructed": false },
        { "name": "Out-of-domain (train HotpotQA → test AmbigQA)", "task_type": "OOD generalization", "newly_constructed": true }
      ],
      "main_metrics": ["Accuracy", "EM", "F1"]
    },

    "key_results": [
      {
        "result_type": "end_to_end_main_table",
        "benchmark": "HotpotQA",
        "metric": "F1",
        "baseline": "RetRobust (best baseline by F1 in Table 1)",
        "baseline_value": "46.49",
        "paper_value": "48.29",
        "delta": "+1.80",
        "notes": "Contriever retriever; Llama-3-8B-Instruct backbone."
      },
      {
        "result_type": "end_to_end_main_table",
        "benchmark": "2WikiMultihopQA",
        "metric": "F1",
        "baseline": "RetRobust (best baseline by F1 in Table 1)",
        "baseline_value": "44.51",
        "paper_value": "46.40",
        "delta": "+1.89",
        "notes": "Contriever retriever; Llama-3-8B-Instruct backbone."
      },
      {
        "result_type": "end_to_end_main_table",
        "benchmark": "AmbigQA",
        "metric": "F1",
        "baseline": "Rewrite-Retrieve-Read (best baseline by F1 in Table 1)",
        "baseline_value": "45.92",
        "paper_value": "48.59",
        "delta": "+2.67",
        "notes": "Contriever retriever; Llama-3-8B-Instruct backbone."
      },
      {
        "result_type": "joint_training_gain",
        "benchmark": "HotpotQA",
        "metric": "F1 (SFT → MAPPO)",
        "baseline": "SFT warm-start (QR+S+G)",
        "baseline_value": "44.69",
        "paper_value": "48.29",
        "delta": "+3.60",
        "notes": "Shows benefit of MAPPO joint optimization over SFT for full 3-agent configuration."
      },
      {
        "result_type": "retriever_robustness",
        "benchmark": "AmbigQA (Retriever: E5)",
        "metric": "F1",
        "baseline": "RAG-DDR (best baseline by F1 in Table 7)",
        "baseline_value": "59.75",
        "paper_value": "60.80",
        "delta": "+1.05",
        "notes": "Demonstrates method remains competitive when swapping the retriever."
      },
      {
        "result_type": "out_of_domain",
        "benchmark": "Train HotpotQA → Test AmbigQA",
        "metric": "F1",
        "baseline": "RetRobust",
        "baseline_value": "44.08",
        "paper_value": "45.62",
        "delta": "+1.54",
        "notes": "OOD generalization result (Table 8)."
      }
    ]
  },

  "Figures": {
    "figures": {
      "architecture_figure": {
        "figure_id": "Figure 1",
        "what_it_shows": "Overall MMOA-RAG framework: Query Rewriter → Retriever → Selector → Generator, trained with shared reward."
      },
      "major_experiment_figures": [
        { "figure_id": "Table 1", "what_it_shows": "Main end-to-end results (Acc/EM/F1) on HotpotQA, 2WikiMultihopQA, AmbigQA with Contriever." },
        { "figure_id": "Figure 2", "what_it_shows": "Ablation on optimizing different agents (exclude QR/S/G) showing full joint optimization performs best." },
        { "figure_id": "Figure 3", "what_it_shows": "Training curves on AmbigQA vs number of training samples (shared reward/F1)." },
        { "figure_id": "Table 2", "what_it_shows": "Generality across module configurations (QR+S+G vs S+G vs QR+G), comparing SFT vs MAPPO." },
        { "figure_id": "Table 7", "what_it_shows": "Robustness when swapping retrievers to BGE and E5." },
        { "figure_id": "Table 8", "what_it_shows": "Out-of-domain generalization (train HotpotQA → test AmbigQA)." },
        { "figure_id": "Figure 4", "what_it_shows": "Heuristic construction process for Selector SFT labels." }
      ]
    }
  }
}
